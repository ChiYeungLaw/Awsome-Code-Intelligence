# Code Intelligence

üíª Welcome to the world of Code Intelligence!

Code Intelligence is an exciting field focused on automating code completion and generation. The ultimate objective is to develop intelligent models capable of generating code based on specific requirements. This repository serves as a comprehensive collection of the latest research and advancements in this domain.

## üéÜ Foundation Models for Code

- Phi-1: Textbooks Are All You Need ([paper](https://arxiv.org/abs/2306.11644), 2023,  *close-source* ‚ùå)
- üî•**WizardCoder**: Empowering Code Large Language Models with Evol-Instruct ([github](https://github.com/nlpxucan/WizardLM/tree/main/WizardCoder), [paper](https://arxiv.org/abs/2306.08568), 2023, *open-source* ‚≠ï) 
- CodeT5+: Open Code Large Language Models for Code Understanding and Generation ([github](https://github.com/salesforce/CodeT5), [paper](https://arxiv.org/abs/2305.07922), 2023, *open-source* ‚≠ï)
- **StarCoder**: May the source be with you! ([github](https://github.com/bigcode-project/starcoder), [paper](https://drive.google.com/file/d/1cN-b9GnWtHzQRoE7M7gAEyivY0kl4BYs/view), 2023, *open-source* ‚≠ï)
- CodeGen2: Lessons for Training LLMs on Programming and Natural Languages ([github](https://github.com/salesforce/CodeGen2), [paper](https://arxiv.org/abs/2305.02309), 2023, *open-source* ‚≠ï)
- Replit-code-v1-3b ([github](https://github.com/replit/ReplitLM/tree/main/replit-code-v1-3b), [twitter](https://twitter.com/Replit/status/1651344184593506304), 2023, *open-source* ‚≠ï)
- GPT4 ([paper](https://arxiv.org/abs/2303.08774), 2023, *close-source* ‚ùå)
- SantaCoder: don't reach for the stars! ([github](https://huggingface.co/bigcode/santacoder), [paper](https://arxiv.org/abs/2301.03988), 2022, *open-source* ‚≠ï)
- CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X ([github](https://github.com/THUDM/CodeGeeX#codegeex-a-multilingual-code-generation-model), [paper](https://arxiv.org/abs/2303.17568), 2022, *open-source* ‚≠ï)
- CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis ([github](https://github.com/salesforce/CodeGen), [paper](https://arxiv.org/pdf/2203.13474.pdf), 2022, *open-source* ‚≠ï)
- Codex: Evaluating Large Language Models Trained on Code (2021, *close-source* ‚ùå)

## üìà Leaderboard on HumanEval for Open-Source Models

| Model            | HumanEval Pass@1 |
|------------------|------------------|
| CodeGen-16B-Multi| 18.3             |
| CodeGeeX         | 22.9             |
| LLaMA-33B        | 21.7             |
| LLaMA-65B        | 23.7             |
| CodeGen-16B-Mono | 29.3             |
| StarCoder-15B    | 33.6             |
| InstructCodeT5+  | 35.0             |
| WizardLM-30B  1.0| 37.8             |
| WizardCoder-15B  1.0 | **57.3**     |


## üî® Training Methods for Code LLMs

- Tuning Models of Code with Compiler-Generated Reinforcement Learning Feedback ([paper](https://arxiv.org/abs/2305.18341), 2023)
- CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning ([github](https://github.com/salesforce/CodeRL), [paper](https://arxiv.org/abs/2207.01780), 2022)

## üîß Prompt Engineering for Code LLMs

- Demystifying GPT Self-Repair for Code Generation ([paper](https://arxiv.org/abs//2306.09896), 2023)
- Think Outside the Code: Brainstorming Boosts Large Language Models in Code Generation ([paper](https://arxiv.org/pdf/2305.10679.pdf), 2023)
- Teaching Large Language Models to Self-Debug ([paper](https://arxiv.org/abs/2304.05128), 2023)
- CodeT: Code Generation with Generated Tests ([github](https://github.com/microsoft/CodeT), [paper](https://arxiv.org/abs/2207.10397), 2022)

## üìã Benchmarks

- [HumanEval](https://github.com/openai/human-eval)
- [MBPP](https://github.com/google-research/google-research/tree/master/mbpp)
- [MultiPL-E](https://github.com/nuprl/MultiPL-E)
- [HumanEval-Plus](https://github.com/evalplus/evalplus)
- [DS-1000](https://ds1000-code-gen.github.io/)
